{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-08-27 20:55:11,641] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible observations:  Discrete(16)\n",
      "possible actions:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('FrozenLake8x8-v0')\n",
    "\n",
    "# 0-left 1-down 2-right 3-up\n",
    "observations = env.observation_space\n",
    "actions_dict = {0: 'left', 1: 'down', 2: 'right', 3: 'up'}\n",
    "actions = env.action_space\n",
    "print('possible observations: ', observations)\n",
    "print('possible actions: ', actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)\n",
    "\n",
    "# q-learning hyper parameter\n",
    "gamma = 0.95  # Discounting factor\n",
    "alpha = 0.8  # soft update param\n",
    "epsilon = 0.1  # 10% chances to apply a random action\n",
    "\n",
    "n_episode = 10000\n",
    "MAX_STEP = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(s, r, a, s_next, done):\n",
    "    max_q_next = max([Q[s_next, a] for a in range(actions.n)])\n",
    "    # Do not include the next state's value if currently at the terminal state.\n",
    "    Q[s, a] += alpha * (r + gamma * max_q_next * (1.0 - done) - Q[s, a])\n",
    "\n",
    "def act(state, episode):\n",
    "#     if np.random.random() < epsilon:\n",
    "#         # action_space.sample() is a convenient function to get a random action\n",
    "#         # that is compatible with this given action space.\n",
    "#         return env.action_space.sample()\n",
    "\n",
    "    # Pick the action with highest q value.\n",
    "    qvals = {a: Q[state, a] + np.random.randn()*(1./(episode+1)) for a in range(actions.n)}\n",
    "    max_q = max(qvals.values())\n",
    "    # In case multiple actions have the same maximum q value.\n",
    "    actions_with_max_q = [a for a, q in qvals.items() if q == max_q]\n",
    "    return np.random.choice(actions_with_max_q)\n",
    "\n",
    "def optimal_act(state):\n",
    "    # Pick the action with highest q value.\n",
    "    qvals = {a: Q[state, a] for a in range(actions.n)}\n",
    "    max_q = max(qvals.values())\n",
    "    # In case multiple actions have the same maximum q value.\n",
    "    actions_with_max_q = [a for a, q in qvals.items() if q == max_q]\n",
    "    return np.random.choice(actions_with_max_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_avg = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Q = defaultdict(float)\n",
    "reward_list = []\n",
    "for episode in range(n_episode):\n",
    "    observation = env.reset()\n",
    "    total_reward = 0\n",
    "    for step in range(MAX_STEP):\n",
    "#         env.render()\n",
    "        action = act(observation, episode)\n",
    "        observation_next, reward, done, info = env.step(action)\n",
    "        # print(observation_next, reward, done, info)\n",
    "        total_reward += reward\n",
    "        update_Q(observation, reward, action, observation_next, done)\n",
    "        observation = observation_next\n",
    "        if done:\n",
    "            # print('-'*10+'end'+'-'*10)\n",
    "            break\n",
    "    reward_list.append(total_reward)\n",
    "#     if episode%n_avg == 0:\n",
    "#         print('Avg reward for {:d}-{:d} episode: {:f}'.format(episode-n_avg, episode, np.average(reward_list[episode-n_avg:episode])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test reward for 1000 episode: 0.752000\n"
     ]
    }
   ],
   "source": [
    "test_episode = 1000\n",
    "test_reward_list = []\n",
    "test_reward = 0\n",
    "observation = env.reset()\n",
    "for episode in range(test_episode):\n",
    "    observation = env.reset()\n",
    "    test_reward = 0\n",
    "    for step in range(MAX_STEP):\n",
    "        # env.render()\n",
    "        action = optimal_act(observation)\n",
    "        observation_next, reward, done, info = env.step(action)\n",
    "        test_reward += reward\n",
    "        observation = observation_next\n",
    "        if done:\n",
    "#             print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "#             print('Test reward: ', test_reward)\n",
    "            observation = env.reset()\n",
    "            break\n",
    "    test_reward_list.append(test_reward)\n",
    "    \n",
    "print('Avg test reward for {:d} episode: {:f}'.format(test_episode, np.average(test_reward_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
